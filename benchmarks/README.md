# API Generation Agent Benchmark

This document provides instructions on how to set up, run, and interpret the results of the API Generation Agent benchmark.

## Overview

The benchmark tool (`benchmark_runner.py`) is designed to evaluate the performance of different Large Language Models (LLMs) in generating API test frameworks. It automates the process of running the API generation agent against a given OpenAPI specification for a list of specified LLMs and collects quantifiable metrics for each run.

## Metrics Collected

For each LLM, the benchmark collects the following metrics:

- **LLM Model Value**: The specific model string used by the underlying libraries (e.g., `gpt-4o`). This corresponds to the value of the `Model` enum member (e.g., `Model.GPT_4_O.value`).
- **Status**: Indicates whether the benchmark run for this LLM was `COMPLETED` or `FAILED`.
- **Error Message**: If the status is `FAILED`, this field will contain the error message.
- **Framework Output Path**: The directory where the generated framework and tests for this LLM are stored.
- **Duration (seconds)**: The total time taken to run the benchmark for this LLM, in seconds. The summary report displays this in a human-readable format (e.g., Xh Ym Zs).
- **Metrics Details**:
    - `generated_test_files_count`: Total number of test script files generated by the agent.
    - `skipped_compilation_files_count`: Number of generated test script files that had TypeScript compilation errors and were skipped.
    - `runnable_test_files_count`: Number of test script files that were successfully compiled and deemed runnable (`generated_test_files_count` - `skipped_compilation_files_count`).
    - `total_tests_executed`: The total number of individual test cases (e.g., `it(...)` blocks) executed by the test runner (Mocha).
    - `passed_tests`: The number of executed test cases that passed.
    - `review_tests`: The number of executed test cases that failed or were otherwise flagged for review (`total_tests_executed` - `passed_tests`).
- **LLM Usage Metadata** (available in the JSON report, summary in the text report):
    - `total_input_tokens`: Total number of input tokens processed by the LLM.
    - `total_output_tokens`: Total number of output tokens generated by the LLM.
    - `total_tokens`: Sum of total input and output tokens.
    - `total_cost`: Estimated cost for the LLM usage during the benchmark run, in USD.
    - `total_cache_details`: Contains `cache_read` (tokens read from cache) and `cache_creation` (tokens written to cache) counts.
    - `call_details`: A list detailing token usage and cost for each individual call to the LLM.

## Setup

1.  **Prerequisites**:
    *   Ensure Python 3.8+ is installed.
    *   Ensure Node.js and npm are installed (for TypeScript compilation and test execution).
    *   The project dependencies must be installed. If you have a `requirements.txt`:
        ```bash
        pip install -r requirements.txt
        ```
    *   Ensure the necessary API keys for the LLMs you intend to benchmark are set as environment variables. Typically, these are `OPENAI_API_KEY` and/or `ANTHROPIC_API_KEY`. These should be accessible in a `.env` file at the root of the project, which `benchmark_runner.py` will load.
        Example `.env` file content:
        ```
        OPENAI_API_KEY="your_openai_api_key"
        ANTHROPIC_API_KEY="your_anthropic_api_key"
        ```

2.  **OpenAPI Specification**:
    *   Have an OpenAPI specification file (JSON or YAML format) ready. This will be the input for the agent.

## Execution

Navigate to the project root directory in your terminal.

The benchmark is run using the `benchmark_runner.py` script located in the `benchmarks/` directory.

> **Note on Parallelism**: The benchmark tool runs the tests for each specified LLM in parallel to expedite the overall execution time. Each LLM's generation and test process is independent. For this reason, the logs in the terminal output can be confusing. Refer to the specific log file for each execution in the /logs folder.

```bash
python benchmarks/benchmark_runner.py --openapi-spec <path_to_openapi_spec> --llms <LLM_LIST> [OPTIONS]
```

**Command-Line Arguments**:

*   `--openapi-spec` (Required): Path to the OpenAPI specification file (e.g., `path/to/your/api.yaml`).
*   `--llms` (Required): A comma-separated list of LLM models to benchmark. Do not use spaces between names.
    *   Available choices: `GPT_5`, `GPT_4_O`, `GPT_4_1`, `O3`, `O4_MINI`, `CLAUDE_SONNET_3_5`, `CLAUDE_SONNET_3_7`, `CLAUDE_SONNET_4` (these are derived from the `Model` enum in `src/configuration/models.py`).
    *   Example: `GPT_5,CLAUDE_SONNET_4`
*   `--endpoints` (Optional): Specific endpoints to target from the OpenAPI specification. If not provided, all endpoints will be targeted.
    *   Example: `/users/{id}`
*   `--output-dir` (Optional): Directory to save benchmark reports and generated frameworks.
    *   Default: `benchmarks/reports`
    *   Generated frameworks for each LLM will be placed in the default agent subdirectory: `generated/`.
*   `--load-results` (Optional): Path to a previously generated JSON report file to load results from. If provided, the benchmark generation and execution steps are skipped. This is primarily intended for testing and debugging the benchmark reporting functionality.
    *   Example: `--load-results benchmarks/reports/benchmark_report_20231026_120000.json`

**Example Usage**:

```bash
python benchmarks/benchmark_runner.py --openapi-spec http://localhost:3000/swagger.json --endpoints /adopters --llms GPT_5,CLAUDE_SONNET_4 --output-dir ./benchmark_run_results
```

This command will run the benchmark using the `http://localhost:3000/swagger.json` API definition, targeting the `/adopters` endpoint for the `GPT_5` and `CLAUDE_SONNET_4` models, saving results to the `./benchmark_run_results` directory.

> **Important Note on Cost**: Running benchmarks, especially with multiple LLMs and/or large OpenAPI specifications (i.e., many endpoints), can incur significant costs due to API calls to the language models. It is highly recommended to start by targeting a single endpoint or a small set of endpoints, as shown in the example above, to understand the potential cost before running benchmarks against an entire API specification. Refer to your LLM provider's pricing page for details on token costs.

## Interpreting Results

Upon completion, the benchmark tool generates a detailed JSON report and prints a summary table to the console:

1.  **JSON Report** (`benchmark_report_<timestamp>.json`):
    *   This file contains a detailed JSON array with the results for each LLM benchmarked.
    *   Each object in the array corresponds to an LLM and includes all the metrics listed in the "Metrics Collected" section.
    *   This file is suitable for programmatic analysis.

2.  **CSV Report** (`benchmark_summary_<timestamp>.csv`):
    *   This file contains a summary of the benchmark results in CSV format, similar to the console output table.
    *   It includes key metrics like LLM Model, test file counts, test execution numbers, duration, token usage, and cost.
    *   This file is suitable for quick analysis in spreadsheet software or for simple data ingestion.

3.  **Summary Table (Console Output)**:
    *   A human-readable table is printed directly to the console, summarizing the key results for each LLM.
    *   It includes: LLM Model, Test Files, Skipped Files, Runnable Files, Tests Executed, Passed Tests, Tests for Review, Duration, Input Tokens, Output Tokens, and Total Cost.
    *   This provides a quick overview of the performance and a reference to the detailed JSON report.

**How to Use the Metrics**:

*   **Compare `generated_test_files_count`**: See which LLMs generate more test files for the same API spec.
*   **Analyze `skipped_compilation_files_count`**: A high number here might indicate issues with the LLM's ability to generate syntactically correct TypeScript or adhere to the project structure.
*   **Focus on `runnable_test_files_count`**: This is a key indicator of how many potentially useful test suites were created.
*   **Evaluate `total_tests_executed`, `passed_tests`, and `review_tests`**: These numbers give insight into the quality and correctness of the generated tests. A high number of `review_tests` suggests that while tests were generated, they might not be valid or might require manual correction.
*   **Analyze `Duration`**: Compare how long each LLM takes to complete the generation and testing process.
*   **Inspect `total_input_tokens`, `total_output_tokens`, and `total_cost`**: These metrics help in understanding the verbosity and cost-effectiveness of different LLMs for the generation task. The detailed `call_details` in the JSON report can provide further insights into token consumption patterns.
*   **Check `Status` and `Error Message`**: For any `FAILED` runs, these fields will help diagnose problems specific to that LLM or its configuration.
*   **Inspect `Framework Output Path`**: Manually review the generated code in this directory to qualitatively assess the structure, readability, and comprehensiveness of the tests generated by each LLM.

By comparing these metrics across different LLMs, you can gain insights into their relative strengths and weaknesses for the task of API test generation within this specific agent's framework.

## Troubleshooting

*   **API Key Errors**: Ensure your `.env` file is correctly set up in the project root and contains valid API keys for the LLMs you are testing.
*   **Dependency Issues**: Double-check that all Python and Node.js dependencies are installed.
*   **File Path Errors**: Verify that the path to your OpenAPI specification is correct.
*   **TypeScript Compilation Errors**: If many files are skipped, it might point to systemic issues in how an LLM generates code. Review the generated files in the output directory for that LLM. 